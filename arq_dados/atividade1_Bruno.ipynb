{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brunoribas68/POS_IA_UFPR/blob/main/arq_dados/atividade1_Bruno.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Identificador automático de idioma</h1>\n",
        "<p><b>Problema</b>: Dados um texto de entrada, é possível identificar em qual língua o texto está escrito?</p>\n",
        "<p>\n",
        "Entrada: \"texto qualquer\" <br />\n",
        "Saída: português ou inglês ou francês ou italiano ou...\n",
        "</p>\n",
        "<p>&nbsp;</p>\n",
        "\n",
        "<h2>O processo de Reconhecimento de Padrões</h2>\n",
        "<p>O objetivo desse trabalho é demonstrar o processo de \"construção de atributos\" e como ele é fundamental para o <b>Reconhecimento de Padrões (RP)</b>.</p>\n",
        "\n",
        "<p>Primeiro um conjunto de \"amostras\" previamente conhecido (classificado)</p>"
      ],
      "metadata": {
        "id": "LPNR0nNzXkji"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OTGqEfJTZrY"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# amostras de texto em diferentes línguas\n",
        "#\n",
        "english = [\n",
        "\"Hello, how are you?\",\n",
        "\"I love to read books.\",\n",
        "\"The weather is nice today.\",\n",
        "\"Where is the nearest restaurant?\",\n",
        "\"What time is it?\",\n",
        "\"I enjoy playing soccer.\",\n",
        "\"Can you help me with this?\",\n",
        "\"I'm going to the movies tonight.\",\n",
        "\"This is a beautiful place.\",\n",
        "\"I like listening to music.\",\n",
        "\"Do you speak English?\",\n",
        "\"What is your favorite color?\",\n",
        "\"I'm learning to play the guitar.\",\n",
        "\"Have a great day!\",\n",
        "\"I need to buy some groceries.\",\n",
        "\"Let's go for a walk.\",\n",
        "\"How was your weekend?\",\n",
        "\"I'm excited for the concert.\",\n",
        "\"Could you pass me the salt, please?\",\n",
        "\"I have a meeting at 2 PM.\",\n",
        "\"I'm planning a vacation.\",\n",
        "\"She sings beautifully.\",\n",
        "\"The cat is sleeping.\",\n",
        "\"I want to learn French.\",\n",
        "\"I enjoy going to the beach.\",\n",
        "\"Where can I find a taxi?\",\n",
        "\"I'm sorry for the inconvenience.\",\n",
        "\"I'm studying for my exams.\",\n",
        "\"I like to cook dinner at home.\",\n",
        "\"Do you have any recommendations for restaurants?\",\n",
        "]\n",
        "\n",
        "spanish = [\n",
        "\"Hola, ¿cómo estás?\",\n",
        "\"Me encanta leer libros.\",\n",
        "\"El clima está agradable hoy.\",\n",
        "\"¿Dónde está el restaurante más cercano?\",\n",
        "\"¿Qué hora es?\",\n",
        "\"Voy al parque todos los días.\",\n",
        "\"¿Puedes ayudarme con esto?\",\n",
        "\"Me gustaría ir de vacaciones.\",\n",
        "\"Este es mi libro favorito.\",\n",
        "\"Me gusta bailar salsa.\",\n",
        "\"¿Hablas español?\",\n",
        "\"¿Cuál es tu comida favorita?\",\n",
        "\"Estoy aprendiendo a tocar el piano.\",\n",
        "\"¡Que tengas un buen día!\",\n",
        "\"Necesito comprar algunas frutas.\",\n",
        "\"Vamos a dar un paseo.\",\n",
        "\"¿Cómo estuvo tu fin de semana?\",\n",
        "\"Estoy emocionado por el concierto.\",\n",
        "\"¿Me pasas la sal, por favor?\",\n",
        "\"Tengo una reunión a las 2 PM.\",\n",
        "\"Estoy planeando unas vacaciones.\",\n",
        "\"Ella canta hermosamente.\",\n",
        "\"El perro está jugando.\",\n",
        "\"Quiero aprender italiano.\",\n",
        "\"Disfruto ir a la playa.\",\n",
        "\"¿Dónde puedo encontrar un taxi?\",\n",
        "\"Lamento las molestias.\",\n",
        "\"Estoy estudiando para mis exámenes.\",\n",
        "\"Me gusta cocinar la cena en casa.\",\n",
        "\"¿Tienes alguna recomendación de restaurantes?\",\n",
        "]\n",
        "\n",
        "portuguese = [\n",
        "\"Estou indo para o trabalho agora.\",\n",
        "\"Adoro passar tempo com minha família.\",\n",
        "\"Preciso comprar leite e pão.\",\n",
        "\"Vamos ao cinema no sábado.\",\n",
        "\"Gosto de praticar esportes ao ar livre.\",\n",
        "\"O trânsito está terrível hoje.\",\n",
        "\"A comida estava deliciosa!\",\n",
        "\"Você já visitou o Rio de Janeiro?\",\n",
        "\"Tenho uma reunião importante amanhã.\",\n",
        "\"A festa começa às 20h.\",\n",
        "\"Estou cansado depois de um longo dia de trabalho.\",\n",
        "\"Vamos fazer um churrasco no final de semana.\",\n",
        "\"O livro que estou lendo é muito interessante.\",\n",
        "\"Estou aprendendo a cozinhar pratos novos.\",\n",
        "\"Preciso fazer exercícios físicos regularmente.\",\n",
        "\"Vou viajar para o exterior nas férias.\",\n",
        "\"Você gosta de dançar?\",\n",
        "\"Hoje é meu aniversário!\",\n",
        "\"Gosto de ouvir música clássica.\",\n",
        "\"Estou estudando para o vestibular.\",\n",
        "\"Meu time de futebol favorito ganhou o jogo.\",\n",
        "\"Quero aprender a tocar violão.\",\n",
        "\"Vamos fazer uma viagem de carro.\",\n",
        "\"O parque fica cheio aos finais de semana.\",\n",
        "\"O filme que assisti ontem foi ótimo.\",\n",
        "\"Preciso resolver esse problema o mais rápido possível.\",\n",
        "\"Adoro explorar novos lugares.\",\n",
        "\"Vou visitar meus avós no domingo.\",\n",
        "\"Estou ansioso para as férias de verão.\",\n",
        "\"Gosto de fazer caminhadas na natureza.\",\n",
        "\"O restaurante tem uma vista incrível.\",\n",
        "\"Vamos sair para jantar no sábado.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A \"amostras\" de texto precisa ser \"transformada\" em <b>padrões</b>\n",
        "\n",
        "<p>Um padrão é um conjunto de características, geralmente representado por um vetor e um conjunto de padrões no formato de tabela. Onde cada linha é um padrão e as colunas as características e, geralmente, na última coluna a <b>classe</b></p>"
      ],
      "metadata": {
        "id": "iY9ZVUEkjkw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = english + spanish + portuguese\n",
        "labels = [\"english\"] * len(english) + [\"spanish\"] * len(spanish) + [\"portuguese\"] * len(portuguese)\n"
      ],
      "metadata": {
        "id": "X9aG5skikHSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O DataFrame do pandas facilita a visualização."
      ],
      "metadata": {
        "id": "82bo5DiGk1aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.decomposition import PCA\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Baixar recursos do NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Função de extração de características\n",
        "def extract_features(text, language):\n",
        "    features = {}\n",
        "    char_counter = Counter(text)\n",
        "    total_chars = sum(char_counter.values())\n",
        "\n",
        "    # Frequência de caracteres\n",
        "    for char, count in char_counter.items():\n",
        "        features[f'char_freq_{char}'] = count / total_chars\n",
        "\n",
        "    # Frequência de palavras\n",
        "    words = word_tokenize(text.lower())\n",
        "    word_counter = Counter(words)\n",
        "    total_words = sum(word_counter.values())\n",
        "    for word, count in word_counter.items():\n",
        "        features[f'word_freq_{word}'] = count / total_words\n",
        "\n",
        "    # Comprimento médio das palavras\n",
        "    word_lengths = [len(word) for word in words]\n",
        "    features['avg_word_length'] = sum(word_lengths) / len(word_lengths) if word_lengths else 0\n",
        "\n",
        "    # Frequência de caracteres especiais\n",
        "    special_chars = re.findall(r'[\\!\\?\\.,;:\\-\\(\\)\\[\\]]', text)\n",
        "    special_char_counter = Counter(special_chars)\n",
        "    for char, count in special_char_counter.items():\n",
        "        features[f'special_char_freq_{char}'] = count / total_chars\n",
        "\n",
        "    # Comprimento do texto\n",
        "    features['text_length'] = len(text)\n",
        "\n",
        "    # Comprimento médio das sentenças\n",
        "    sentences = sent_tokenize(text)\n",
        "    sentence_lengths = [len(sentence) for sentence in sentences]\n",
        "    features['avg_sentence_length'] = sum(sentence_lengths) / len(sentence_lengths) if sentence_lengths else 0\n",
        "\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "nlBpZ6yvlygB",
        "outputId": "f6f0f6ac-dece-4d67-b37b-4d8f217e340d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Construção dos atributos</h1>\n",
        "<p>Esse é o coração desse trabalho e que deverá ser desenvolvido por vocês. Pensem em como podemos \"medir\" cadas frase/sentença e extrair características que melhorem o resultado do processo de identificação.<p>\n",
        "<p>Após a criação de cada novo atributo, execute as etapas seguintes e registre as métricas da matriz de confusão. Principalmente acurácia e a precisão.</p>"
      ],
      "metadata": {
        "id": "A0owXHcxlP_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extração de características\n",
        "languages = [\"english\"] * len(english) + [\"spanish\"] * len(spanish) + [\"portuguese\"] * len(portuguese)\n",
        "feature_list = [extract_features(text, lang) for text, lang in zip(texts, languages)]\n",
        "df = pd.DataFrame(feature_list).fillna(0)  # Substitui NaN por 0\n",
        "print(df)"
      ],
      "metadata": {
        "id": "PytJtdd1k_F6",
        "outputId": "42650cd1-fefa-4185-c331-5f4dac0e18a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    char_freq_H  char_freq_e  char_freq_l  char_freq_o  char_freq_,  \\\n",
            "0      0.052632     0.105263     0.105263     0.157895     0.052632   \n",
            "1      0.000000     0.095238     0.047619     0.190476     0.000000   \n",
            "2      0.000000     0.153846     0.000000     0.038462     0.000000   \n",
            "3      0.000000     0.187500     0.000000     0.000000     0.000000   \n",
            "4      0.000000     0.062500     0.000000     0.000000     0.000000   \n",
            "..          ...          ...          ...          ...          ...   \n",
            "87     0.000000     0.030303     0.000000     0.121212     0.000000   \n",
            "88     0.000000     0.052632     0.000000     0.105263     0.000000   \n",
            "89     0.000000     0.078947     0.000000     0.052632     0.000000   \n",
            "90     0.000000     0.108108     0.027027     0.000000     0.000000   \n",
            "91     0.000000     0.000000     0.000000     0.090909     0.000000   \n",
            "\n",
            "    char_freq_   char_freq_h  char_freq_w  char_freq_a  char_freq_r  ...  \\\n",
            "0      0.157895     0.052632     0.052632     0.052632     0.052632  ...   \n",
            "1      0.190476     0.000000     0.000000     0.047619     0.047619  ...   \n",
            "2      0.153846     0.076923     0.038462     0.076923     0.038462  ...   \n",
            "3      0.125000     0.062500     0.000000     0.093750     0.125000  ...   \n",
            "4      0.187500     0.062500     0.000000     0.062500     0.000000  ...   \n",
            "..          ...          ...          ...          ...          ...  ...   \n",
            "87     0.151515     0.000000     0.000000     0.060606     0.030303  ...   \n",
            "88     0.157895     0.000000     0.000000     0.131579     0.078947  ...   \n",
            "89     0.131579     0.026316     0.000000     0.184211     0.052632  ...   \n",
            "90     0.135135     0.000000     0.000000     0.108108     0.081081  ...   \n",
            "91     0.151515     0.000000     0.000000     0.212121     0.090909  ...   \n",
            "\n",
            "    word_freq_as  word_freq_verão  word_freq_caminhadas  word_freq_na  \\\n",
            "0          0.000            0.000              0.000000      0.000000   \n",
            "1          0.000            0.000              0.000000      0.000000   \n",
            "2          0.000            0.000              0.000000      0.000000   \n",
            "3          0.000            0.000              0.000000      0.000000   \n",
            "4          0.000            0.000              0.000000      0.000000   \n",
            "..           ...              ...                   ...           ...   \n",
            "87         0.000            0.000              0.000000      0.000000   \n",
            "88         0.125            0.125              0.000000      0.000000   \n",
            "89         0.000            0.000              0.142857      0.142857   \n",
            "90         0.000            0.000              0.000000      0.000000   \n",
            "91         0.000            0.000              0.000000      0.000000   \n",
            "\n",
            "    word_freq_natureza  word_freq_tem  word_freq_vista  word_freq_incrível  \\\n",
            "0             0.000000       0.000000         0.000000            0.000000   \n",
            "1             0.000000       0.000000         0.000000            0.000000   \n",
            "2             0.000000       0.000000         0.000000            0.000000   \n",
            "3             0.000000       0.000000         0.000000            0.000000   \n",
            "4             0.000000       0.000000         0.000000            0.000000   \n",
            "..                 ...            ...              ...                 ...   \n",
            "87            0.000000       0.000000         0.000000            0.000000   \n",
            "88            0.000000       0.000000         0.000000            0.000000   \n",
            "89            0.142857       0.000000         0.000000            0.000000   \n",
            "90            0.000000       0.142857         0.142857            0.142857   \n",
            "91            0.000000       0.000000         0.000000            0.000000   \n",
            "\n",
            "    word_freq_sair  word_freq_jantar  \n",
            "0         0.000000          0.000000  \n",
            "1         0.000000          0.000000  \n",
            "2         0.000000          0.000000  \n",
            "3         0.000000          0.000000  \n",
            "4         0.000000          0.000000  \n",
            "..             ...               ...  \n",
            "87        0.000000          0.000000  \n",
            "88        0.000000          0.000000  \n",
            "89        0.000000          0.000000  \n",
            "90        0.000000          0.000000  \n",
            "91        0.142857          0.142857  \n",
            "\n",
            "[92 rows x 394 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Treinando o modelo com SVM</h1>\n",
        "<p>Separando o conjunto de treinamento do conjunto de testes</p>"
      ],
      "metadata": {
        "id": "Hiy_-Mw6x1HU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Criação de vetores de características e rótulos\n",
        "X = df.values\n",
        "y = labels\n",
        "\n",
        "# Normalização dos dados\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Redução de dimensionalidade\n",
        "pca = PCA(n_components=50)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Divisão dos dados em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "k9JFhvGBx6Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7WNh7U64q1aP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com os conjuntos separados, podemos \"treinar\" o modelo usando a SVM."
      ],
      "metadata": {
        "id": "o3YCjbFm2Gdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "treinador = svm.SVC()  #algoritmo escolhido\n",
        "modelo = treinador.fit(X_train, y_train)\n",
        "\n",
        "#\n",
        "# score com os dados de treinamento\n",
        "acuracia = modelo.score(X_train, y_train)\n",
        "print(\"Acurácia nos dados de treinamento: {:.2f}%\".format(acuracia * 100))\n",
        "\n",
        "#\n",
        "# melhor avaliar com a matriz de confusão\n",
        "y_pred = modelo.predict(X_train)\n",
        "cm = confusion_matrix(y_train, y_pred)\n",
        "print(cm)\n",
        "print(classification_report(y_train, y_pred))\n",
        "\n",
        "#\n",
        "# com dados de teste que não foram usados no treinamento\n",
        "print('métricas mais confiáveis')\n",
        "y_pred2 = modelo.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred2)\n",
        "print(cm)\n",
        "print(classification_report(y_test, y_pred2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ggUrZC42ib4",
        "outputId": "a4f0c3f4-7f27-404a-9d01-c44dff757854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia nos dados de treinamento: 98.63%\n",
            "[[22  0  0]\n",
            " [ 0 26  1]\n",
            " [ 0  0 24]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     english       1.00      1.00      1.00        22\n",
            "  portuguese       1.00      0.96      0.98        27\n",
            "     spanish       0.96      1.00      0.98        24\n",
            "\n",
            "    accuracy                           0.99        73\n",
            "   macro avg       0.99      0.99      0.99        73\n",
            "weighted avg       0.99      0.99      0.99        73\n",
            "\n",
            "métricas mais confiáveis\n",
            "[[7 1 0]\n",
            " [0 5 0]\n",
            " [1 2 3]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     english       0.88      0.88      0.88         8\n",
            "  portuguese       0.62      1.00      0.77         5\n",
            "     spanish       1.00      0.50      0.67         6\n",
            "\n",
            "    accuracy                           0.79        19\n",
            "   macro avg       0.83      0.79      0.77        19\n",
            "weighted avg       0.85      0.79      0.78        19\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2av477FM0pTV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}